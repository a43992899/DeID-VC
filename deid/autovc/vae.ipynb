{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.utils import data\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "\n",
    "import numpy as np\n",
    "import pickle \n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset and dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpkrEmbDataset(data.Dataset):\n",
    "    \"\"\"Dataset class for the Utterances dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, pkl_path=\"/home/yrb/data/ID-DEID_data/voxceleb_spmel/train.pkl\"):\n",
    "        \"\"\"Initialize and preprocess the Utterances dataset.\"\"\"\n",
    "        self.pkl_path = pkl_path\n",
    "        \n",
    "        \"\"\"Load data\"\"\"\n",
    "        self.load_data()\n",
    "        \n",
    "        \n",
    "    def load_data(self):  \n",
    "        # load train.pkl\n",
    "        with open(self.pkl_path, \"rb\") as f:\n",
    "            meta = pickle.load(f)\n",
    "        self.dataset = [(sbmt[0], sbmt[1]) for sbmt in meta] # (spkr_id, spkr_emb)\n",
    "        self.num_spkr = len(self.dataset)\n",
    "        print('Finished loading the dataset...')\n",
    "\n",
    "                   \n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        spkr_id, spkr_emb = self.dataset[index]\n",
    "        return spkr_id, spkr_emb\n",
    "    \n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the number of spkrs.\"\"\"\n",
    "        return self.num_spkr\n",
    "\n",
    "def get_loader(pkl_path=\"/home/yrb/data/ID-DEID_data/voxceleb_spmel/train.pkl\", batch_size=16, num_workers=0, shuffle=True, drop_last=True):\n",
    "    \"\"\"Build and return a data loader.\"\"\"\n",
    "    \n",
    "    dataset = SpkrEmbDataset(pkl_path)\n",
    "    \n",
    "    worker_init_fn = lambda x: np.random.seed((torch.initial_seed()) % (2**32))\n",
    "    data_loader = data.DataLoader(dataset=dataset,\n",
    "                                  batch_size=batch_size,\n",
    "                                  shuffle=shuffle,\n",
    "                                  num_workers=num_workers,\n",
    "                                  drop_last=drop_last,\n",
    "                                  worker_init_fn=worker_init_fn)\n",
    "    return data_loader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VAE model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    A simple implementation of Gaussian MLP Encoder and Decoder\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, latent_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.FC_input = nn.Linear(input_dim, hidden_dim)\n",
    "        self.FC_input2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "        self.FC_mean = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.FC_var = nn.Linear(hidden_dim, latent_dim)\n",
    "\n",
    "        self.LeakyReLU = nn.LeakyReLU(0.2)\n",
    "\n",
    "        self.training = True\n",
    "\n",
    "    def forward(self, x):\n",
    "        h_ = self.LeakyReLU(self.FC_input(x))\n",
    "        h_ = self.LeakyReLU(self.FC_input2(h_))\n",
    "\n",
    "        mean = self.FC_mean(h_)\n",
    "        # encoder produces mean and log of variance\n",
    "        # (i.e., parateters of simple tractable normal distribution \"q\"\n",
    "        log_var = self.FC_var(h_)\n",
    "        return mean, log_var\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, latent_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.FC_input = nn.Linear(input_dim, hidden_dim)\n",
    "        # self.FC_input2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "        self.FC_mean = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.FC_var = nn.Linear(hidden_dim, latent_dim)\n",
    "\n",
    "        self.LeakyReLU = nn.LeakyReLU(0.2)\n",
    "\n",
    "        self.training = True\n",
    "\n",
    "    def forward(self, x):\n",
    "        h_ = self.LeakyReLU(self.FC_input(x))\n",
    "        # h_ = self.LeakyReLU(self.FC_input2(h_))\n",
    "\n",
    "        mean = self.FC_mean(h_)\n",
    "        # encoder produces mean and log of variance\n",
    "        # (i.e., parateters of simple tractable normal distribution \"q\"\n",
    "        log_var = self.FC_var(h_)\n",
    "        return mean, log_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim, hidden_dim, output_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.FC_hidden = nn.Linear(latent_dim, hidden_dim)\n",
    "        self.FC_hidden2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.FC_output = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "        self.LeakyReLU = nn.LeakyReLU(0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.LeakyReLU(self.FC_hidden(x))\n",
    "        h = self.LeakyReLU(self.FC_hidden2(h))\n",
    "\n",
    "        x_hat = torch.tanh(self.FC_output(h))\n",
    "        return x_hat\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim, hidden_dim, output_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.FC_hidden = nn.Linear(latent_dim, hidden_dim)\n",
    "        # self.FC_hidden2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.FC_output = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "        self.LeakyReLU = nn.LeakyReLU(0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.LeakyReLU(self.FC_hidden(x))\n",
    "        # h = self.LeakyReLU(self.FC_hidden2(h))\n",
    "\n",
    "        x_hat = torch.tanh(self.FC_output(h))\n",
    "        return x_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, x_dim, hidden_dim, latent_dim, DEVICE):\n",
    "        super(VAE, self).__init__()\n",
    "        self.Encoder = Encoder(input_dim=x_dim, hidden_dim=hidden_dim, latent_dim=latent_dim)\n",
    "        self.Decoder = Decoder(latent_dim=latent_dim, hidden_dim = hidden_dim, output_dim = x_dim)\n",
    "        self.DEVICE = DEVICE\n",
    "\n",
    "    def reparameterization(self, mean, var):\n",
    "        epsilon = torch.randn_like(var).to(self.DEVICE)  # sampling epsilon\n",
    "        z = mean + var*epsilon  # reparameterization trick\n",
    "        return z\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean, log_var = self.Encoder(x)\n",
    "        # takes exponential function (log var -> var)\n",
    "        z = self.reparameterization(mean, torch.exp(0.5 * log_var))\n",
    "        x_hat = self.Decoder(z)\n",
    "\n",
    "        return x_hat, mean, log_var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENV settings\n",
    "cuda = True\n",
    "DEVICE = torch.device(\"cuda\" if cuda else \"cpu\")\n",
    "\n",
    "# Model Hyperparameters\n",
    "batch_size = 32\n",
    "\n",
    "x_dim = 256\n",
    "hidden_dim = 384\n",
    "latent_dim = 64\n",
    "\n",
    "lr = 1e-3\n",
    "\n",
    "epochs = 60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VAE(x_dim, hidden_dim, latent_dim, DEVICE).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "mse = nn.MSELoss(reduction='sum')\n",
    "l1 = nn.L1Loss(reduction='sum')\n",
    "def loss_function2(x, x_hat, mean, log_var):\n",
    "    # cosine similarity loss\n",
    "    cos_distance_loss = 100*(1-cos(x, x_hat)).sum()\n",
    "    mse_loss = mse(x, x_hat)\n",
    "    reconstruction_loss = cos_distance_loss + mse_loss\n",
    "    KLD = - 0.5 * torch.sum(1 + log_var - mean.pow(2) - log_var.exp())\n",
    "    return reconstruction_loss, KLD\n",
    "\n",
    "def loss_function1(x, x_hat, mean, log_var):\n",
    "    # cosine similarity loss\n",
    "    cos_distance_loss = 200*(1-cos(x, x_hat)).sum()\n",
    "    l1_loss = l1(x, x_hat)\n",
    "    reconstruction_loss = cos_distance_loss + l1_loss\n",
    "    KLD = - 0.5 * torch.sum(1 + log_var - mean.pow(2) - log_var.exp())\n",
    "    return reconstruction_loss, KLD\n",
    "\n",
    "def loss_function(x, x_hat, mean, log_var):\n",
    "    mse_loss = mse(x, x_hat)\n",
    "    reconstruction_loss = 10 * mse_loss\n",
    "    KLD = - 0.5 * torch.sum(1 + log_var - mean.pow(2) - log_var.exp())\n",
    "    return reconstruction_loss, KLD\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing the data loader...\n",
      "Finished loading the dataset...\n",
      "Finished loading the dataset...\n",
      "Finished loading the dataset...\n",
      "Start training VAE...\n",
      "\tEpoch 1 complete! \tAverage Loss:  4.989629334637097 \tAverage reconst Loss:  4.956616018499647 \tAverage KLD Loss:  0.033013316170711605\n",
      "\tEpoch 2 complete! \tAverage Loss:  4.236941071493285 \tAverage reconst Loss:  4.228967549545424 \tAverage KLD Loss:  0.007973513274919242\n",
      "\tEpoch 3 complete! \tAverage Loss:  4.1526430036340445 \tAverage reconst Loss:  4.1484784207173755 \tAverage KLD Loss:  0.0041645761604221275\n",
      "\tEpoch 4 complete! \tAverage Loss:  4.114597948534148 \tAverage reconst Loss:  4.111635301794324 \tAverage KLD Loss:  0.002962652219658984\n",
      "\tEpoch 5 complete! \tAverage Loss:  4.098131466124739 \tAverage reconst Loss:  4.0965201152222495 \tAverage KLD Loss:  0.0016113486739673785\n",
      "\tEpoch 6 complete! \tAverage Loss:  4.081492188785758 \tAverage reconst Loss:  4.079987690917084 \tAverage KLD Loss:  0.0015044958438790803\n",
      "\tEpoch 7 complete! \tAverage Loss:  4.072425249431815 \tAverage reconst Loss:  4.070649016116347 \tAverage KLD Loss:  0.0017762431400894587\n",
      "\tEpoch 8 complete! \tAverage Loss:  4.066922444318022 \tAverage reconst Loss:  4.065247554864202 \tAverage KLD Loss:  0.001674890867434442\n",
      "\tEpoch 9 complete! \tAverage Loss:  4.065522125789097 \tAverage reconst Loss:  4.064052871295384 \tAverage KLD Loss:  0.0014692418917547911\n",
      "\tEpoch 10 complete! \tAverage Loss:  4.055724281285491 \tAverage reconst Loss:  4.05401256361178 \tAverage KLD Loss:  0.0017117272530283248\n",
      "\tEpoch 11 complete! \tAverage Loss:  4.053786459778037 \tAverage reconst Loss:  4.052085853048733 \tAverage KLD Loss:  0.0017006039436507439\n",
      "\tEpoch 12 complete! \tAverage Loss:  4.053072540887764 \tAverage reconst Loss:  4.051560449813094 \tAverage KLD Loss:  0.001512087553107579\n",
      "\tEpoch 13 complete! \tAverage Loss:  4.0500551877277235 \tAverage reconst Loss:  4.048447301345212 \tAverage KLD Loss:  0.0016078906815632113\n",
      "\tEpoch 14 complete! \tAverage Loss:  4.047360724636486 \tAverage reconst Loss:  4.045634905142443 \tAverage KLD Loss:  0.0017258087089950486\n",
      "\tEpoch 15 complete! \tAverage Loss:  4.047306700476578 \tAverage reconst Loss:  4.045396094875676 \tAverage KLD Loss:  0.0019106086484888302\n",
      "\tEpoch 16 complete! \tAverage Loss:  4.044703842273781 \tAverage reconst Loss:  4.043339550495148 \tAverage KLD Loss:  0.0013642894045915455\n",
      "\tEpoch 17 complete! \tAverage Loss:  4.042366294988564 \tAverage reconst Loss:  4.041456105453627 \tAverage KLD Loss:  0.0009101846704392561\n",
      "\tEpoch 18 complete! \tAverage Loss:  4.04155936305012 \tAverage reconst Loss:  4.0403236874512265 \tAverage KLD Loss:  0.0012356745594713306\n",
      "\tEpoch 19 complete! \tAverage Loss:  4.043315302048411 \tAverage reconst Loss:  4.042388294424329 \tAverage KLD Loss:  0.0009269989968743175\n",
      "\tEpoch 20 complete! \tAverage Loss:  4.045569641249521 \tAverage reconst Loss:  4.0439767475639075 \tAverage KLD Loss:  0.0015928951200164323\n",
      "\tEpoch 21 complete! \tAverage Loss:  4.046242779919079 \tAverage reconst Loss:  4.044923976063728 \tAverage KLD Loss:  0.0013187950368904109\n",
      "\tEpoch 22 complete! \tAverage Loss:  4.049764595925808 \tAverage reconst Loss:  4.048242947884968 \tAverage KLD Loss:  0.0015216519823297858\n",
      "\tEpoch 23 complete! \tAverage Loss:  4.046092717775276 \tAverage reconst Loss:  4.0445294092808455 \tAverage KLD Loss:  0.0015633220568166248\n",
      "\tEpoch 24 complete! \tAverage Loss:  4.048615545034409 \tAverage reconst Loss:  4.046753494867256 \tAverage KLD Loss:  0.0018620505330285855\n",
      "\tEpoch 25 complete! \tAverage Loss:  4.054771329675402 \tAverage reconst Loss:  4.052230059036186 \tAverage KLD Loss:  0.002541268140443468\n",
      "\tEpoch 26 complete! \tAverage Loss:  4.052163011261395 \tAverage reconst Loss:  4.05026404772486 \tAverage KLD Loss:  0.0018989646216920977\n",
      "\tEpoch 27 complete! \tAverage Loss:  4.056228698364326 \tAverage reconst Loss:  4.053769267031124 \tAverage KLD Loss:  0.002459436472106193\n",
      "\tEpoch 28 complete! \tAverage Loss:  4.05549998155662 \tAverage reconst Loss:  4.05306219628879 \tAverage KLD Loss:  0.002437791762141777\n",
      "\tEpoch 29 complete! \tAverage Loss:  4.054107418017728 \tAverage reconst Loss:  4.051276837076459 \tAverage KLD Loss:  0.00283058564361584\n",
      "\tEpoch 30 complete! \tAverage Loss:  4.060678902481284 \tAverage reconst Loss:  4.056496657431126 \tAverage KLD Loss:  0.004182233878444614\n",
      "\tEpoch 31 complete! \tAverage Loss:  4.0631198436021805 \tAverage reconst Loss:  4.059248087661607 \tAverage KLD Loss:  0.0038717465358786285\n",
      "\tEpoch 32 complete! \tAverage Loss:  4.058924879346575 \tAverage reconst Loss:  4.055726433438914 \tAverage KLD Loss:  0.003198438635860969\n",
      "\tEpoch 33 complete! \tAverage Loss:  4.059146711868899 \tAverage reconst Loss:  4.056462876498699 \tAverage KLD Loss:  0.002683835627976805\n",
      "\tEpoch 34 complete! \tAverage Loss:  4.058525613376072 \tAverage reconst Loss:  4.054006439234529 \tAverage KLD Loss:  0.004519172885920852\n",
      "\tEpoch 35 complete! \tAverage Loss:  4.060477073703494 \tAverage reconst Loss:  4.056981621044023 \tAverage KLD Loss:  0.003495445267097758\n",
      "\tEpoch 36 complete! \tAverage Loss:  4.0583757098232 \tAverage reconst Loss:  4.055010849876063 \tAverage KLD Loss:  0.003364857307003279\n",
      "\tEpoch 37 complete! \tAverage Loss:  4.058455342692988 \tAverage reconst Loss:  4.055153416735785 \tAverage KLD Loss:  0.0033019246849497513\n",
      "\tEpoch 38 complete! \tAverage Loss:  4.056067802011967 \tAverage reconst Loss:  4.054151386022568 \tAverage KLD Loss:  0.0019164090127950268\n",
      "\tEpoch 39 complete! \tAverage Loss:  4.056630393224103 \tAverage reconst Loss:  4.052663180444922 \tAverage KLD Loss:  0.003967212737604443\n",
      "\tEpoch 40 complete! \tAverage Loss:  4.060853812311377 \tAverage reconst Loss:  4.055997497269085 \tAverage KLD Loss:  0.004856325756658667\n",
      "\tEpoch 41 complete! \tAverage Loss:  4.061076484620571 \tAverage reconst Loss:  4.057778691606862 \tAverage KLD Loss:  0.0032977941404429396\n",
      "\tEpoch 42 complete! \tAverage Loss:  4.063025600143841 \tAverage reconst Loss:  4.059201329946518 \tAverage KLD Loss:  0.0038242552005353253\n",
      "\tEpoch 43 complete! \tAverage Loss:  4.0610605371849875 \tAverage reconst Loss:  4.056695061070578 \tAverage KLD Loss:  0.004365476904370423\n",
      "\tEpoch 44 complete! \tAverage Loss:  4.058103699769292 \tAverage reconst Loss:  4.054292394646576 \tAverage KLD Loss:  0.003811301268537396\n",
      "\tEpoch 45 complete! \tAverage Loss:  4.0583252268178125 \tAverage reconst Loss:  4.053746193647385 \tAverage KLD Loss:  0.00457902965717949\n",
      "\tEpoch 46 complete! \tAverage Loss:  4.061612805085523 \tAverage reconst Loss:  4.057740760701043 \tAverage KLD Loss:  0.0038720489122040036\n",
      "\tEpoch 47 complete! \tAverage Loss:  4.061121587242399 \tAverage reconst Loss:  4.058038214487689 \tAverage KLD Loss:  0.003083374550832169\n",
      "\tEpoch 48 complete! \tAverage Loss:  4.055766128003597 \tAverage reconst Loss:  4.0519718038184305 \tAverage KLD Loss:  0.0037943217903375626\n",
      "\tEpoch 49 complete! \tAverage Loss:  4.061502782361848 \tAverage reconst Loss:  4.056634097227028 \tAverage KLD Loss:  0.00486868731344917\n",
      "\tEpoch 50 complete! \tAverage Loss:  4.061907993895667 \tAverage reconst Loss:  4.058131604322365 \tAverage KLD Loss:  0.0037763831954050276\n",
      "\tEpoch 51 complete! \tAverage Loss:  4.061086320451328 \tAverage reconst Loss:  4.056758988116469 \tAverage KLD Loss:  0.004327330784040636\n",
      "\tEpoch 52 complete! \tAverage Loss:  4.062620789877006 \tAverage reconst Loss:  4.057621343859604 \tAverage KLD Loss:  0.004999443689095122\n",
      "\tEpoch 53 complete! \tAverage Loss:  4.057908581835883 \tAverage reconst Loss:  4.0540293997951915 \tAverage KLD Loss:  0.0038791818120184223\n",
      "\tEpoch 54 complete! \tAverage Loss:  4.063664590673787 \tAverage reconst Loss:  4.0590172825115065 \tAverage KLD Loss:  0.004647311438540263\n",
      "\tEpoch 55 complete! \tAverage Loss:  4.060522602072784 \tAverage reconst Loss:  4.056893794664314 \tAverage KLD Loss:  0.003628805113424148\n",
      "\tEpoch 56 complete! \tAverage Loss:  4.05930163285562 \tAverage reconst Loss:  4.056478075683117 \tAverage KLD Loss:  0.002823567878554708\n",
      "\tEpoch 57 complete! \tAverage Loss:  4.05781240867717 \tAverage reconst Loss:  4.054178747747626 \tAverage KLD Loss:  0.0036336755770857315\n",
      "\tEpoch 58 complete! \tAverage Loss:  4.062542068106787 \tAverage reconst Loss:  4.058997875877789 \tAverage KLD Loss:  0.0035442032801386502\n",
      "\tEpoch 59 complete! \tAverage Loss:  4.059189001364367 \tAverage reconst Loss:  4.055124061448233 \tAverage KLD Loss:  0.004064946834530149\n",
      "\tEpoch 60 complete! \tAverage Loss:  4.05893230757543 \tAverage reconst Loss:  4.054093497140067 \tAverage KLD Loss:  0.004838814060868961\n",
      "Finish!!\n"
     ]
    }
   ],
   "source": [
    "print(\"Preparing the data loader...\")\n",
    "vox_dataloader = get_loader(pkl_path=\"/home/yrb/data/ID-DEID_data/voxceleb_spmel/train.pkl\", num_workers=2, batch_size=batch_size)\n",
    "wsj_dataloader = get_loader(pkl_path=\"/home/yrb/data/ID-DEID_data/wsj_spmel/train.pkl\", num_workers=2, batch_size=batch_size)\n",
    "vctk_dataloader = get_loader(pkl_path=\"/home/yrb/data/ID-DEID_data/vctk_spmel/train.pkl\", num_workers=2, batch_size=batch_size)\n",
    "\n",
    "dataloader = vox_dataloader\n",
    "\n",
    "print(\"Start training VAE...\")\n",
    "model.train()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    overall_loss, overall_reconst, overall_KLD = 0, 0, 0\n",
    "    for batch_idx, (spkrids, x) in enumerate(dataloader):\n",
    "        x = x.to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        x_hat, mean, log_var = model(x)\n",
    "        reconst_loss, KLD = loss_function(x, x_hat, mean, log_var)\n",
    "        loss = reconst_loss + KLD\n",
    "        overall_loss += loss.item()\n",
    "        overall_reconst += reconst_loss.item()\n",
    "        overall_KLD += KLD.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(\"\\tEpoch\", epoch + 1, \"complete!\",\n",
    "        \"\\tAverage Loss: \", overall_loss / (batch_idx*batch_size),\n",
    "        \"\\tAverage reconst Loss: \", overall_reconst / (batch_idx*batch_size),\n",
    "        \"\\tAverage KLD Loss: \", overall_KLD / (batch_idx*batch_size),)\n",
    "\n",
    "print(\"Finish!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vox_dataloader = get_loader(pkl_path=\"/home/yrb/data/ID-DEID_data/voxceleb_spmel/train.pkl\", num_workers=2, batch_size=batch_size, shuffle=False, drop_last=False)\n",
    "wsj_dataloader = get_loader(pkl_path=\"/home/yrb/data/ID-DEID_data/wsj_spmel/train.pkl\", num_workers=2, batch_size=batch_size, shuffle=False, drop_last=False)\n",
    "vctk_dataloader = get_loader(pkl_path=\"/home/yrb/data/ID-DEID_data/vctk_spmel/train.pkl\", num_workers=2, batch_size=batch_size, shuffle=False, drop_last=False)\n",
    "\n",
    "\n",
    "def eval(model, dataloader):\n",
    "    model.eval()\n",
    "    cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "    cos_sim = 0.0\n",
    "    mse = 0.0\n",
    "    n_sample = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (spkrids, x) in enumerate(dataloader):\n",
    "            x = x.to(DEVICE)\n",
    "            n_sample += x.shape[0]\n",
    "            x_hat, _, _ = model(x)\n",
    "            cos_sim += cos(x_hat, x).sum()\n",
    "            mse += ((x-x_hat)**2).sum()\n",
    "    return {\"cos_sim\": cos_sim/n_sample, \"mse\": mse/n_sample}\n",
    "\n",
    "print(\"wsj:\")\n",
    "print(\"model:\", eval(model, wsj_dataloader))\n",
    "\n",
    "print(\"vox:\")\n",
    "print(\"model:\", eval(model, vox_dataloader))\n",
    "\n",
    "print(\"vctk:\")\n",
    "print(\"model:\", eval(model, vctk_dataloader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading the dataset...\n",
      "Finished loading the dataset...\n",
      "Finished loading the dataset...\n",
      "wsj:\n",
      "model1: {'cos_sim': tensor(0.5309, device='cuda:0'), 'mse': tensor(0.5145, device='cuda:0')}\n",
      "model2: {'cos_sim': tensor(0.4445, device='cuda:0'), 'mse': tensor(0.8494, device='cuda:0')}\n",
      "model: {'cos_sim': tensor(0.0331, device='cuda:0'), 'mse': tensor(0.6336, device='cuda:0')}\n",
      "vox:\n",
      "model1: {'cos_sim': tensor(0.9164, device='cuda:0'), 'mse': tensor(0.0834, device='cuda:0')}\n",
      "model2: {'cos_sim': tensor(0.8696, device='cuda:0'), 'mse': tensor(0.4103, device='cuda:0')}\n",
      "model: {'cos_sim': tensor(0.1941, device='cuda:0'), 'mse': tensor(0.4037, device='cuda:0')}\n",
      "vctk:\n",
      "model1: {'cos_sim': tensor(0.5026, device='cuda:0'), 'mse': tensor(0.5190, device='cuda:0')}\n",
      "model2: {'cos_sim': tensor(0.4478, device='cuda:0'), 'mse': tensor(0.9651, device='cuda:0')}\n",
      "model: {'cos_sim': tensor(0.0831, device='cuda:0'), 'mse': tensor(0.6302, device='cuda:0')}\n"
     ]
    }
   ],
   "source": [
    "vox_dataloader = get_loader(pkl_path=\"/home/yrb/data/ID-DEID_data/voxceleb_spmel/train.pkl\", num_workers=2, batch_size=batch_size, shuffle=False, drop_last=False)\n",
    "wsj_dataloader = get_loader(pkl_path=\"/home/yrb/data/ID-DEID_data/wsj_spmel/train.pkl\", num_workers=2, batch_size=batch_size, shuffle=False, drop_last=False)\n",
    "vctk_dataloader = get_loader(pkl_path=\"/home/yrb/data/ID-DEID_data/vctk_spmel/train.pkl\", num_workers=2, batch_size=batch_size, shuffle=False, drop_last=False)\n",
    "\n",
    "model1 = VAE(256, 384, 64, DEVICE).to(DEVICE)\n",
    "checkpoint = torch.load(\"/home/yrb/code/ID-DEID/data/vae_model/l1_vae_on_voxceleb.ckpt\", map_location=DEVICE)\n",
    "model1.load_state_dict(checkpoint['model'])\n",
    "\n",
    "model2 = VAE(256, 384, 64, DEVICE).to(DEVICE)\n",
    "checkpoint = torch.load(\"/home/yrb/code/ID-DEID/data/vae_model/vae_on_voxceleb.ckpt\", map_location=DEVICE)\n",
    "model2.load_state_dict(checkpoint['model'])\n",
    "\n",
    "\n",
    "def eval(model, dataloader):\n",
    "    model.eval()\n",
    "    cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "    cos_sim = 0.0\n",
    "    mse = 0.0\n",
    "    n_sample = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (spkrids, x) in enumerate(dataloader):\n",
    "            x = x.to(DEVICE)\n",
    "            n_sample += x.shape[0]\n",
    "            x_hat, _, _ = model(x)\n",
    "            cos_sim += cos(x_hat, x).sum()\n",
    "            mse += ((x-x_hat)**2).sum()\n",
    "    return {\"cos_sim\": cos_sim/n_sample, \"mse\": mse/n_sample}\n",
    "\n",
    "print(\"wsj:\")\n",
    "print(\"model1:\", eval(model1, wsj_dataloader))\n",
    "print(\"model2:\", eval(model2, wsj_dataloader))\n",
    "print(\"model:\", eval(model, wsj_dataloader))\n",
    "\n",
    "print(\"vox:\")\n",
    "print(\"model1:\", eval(model1, vox_dataloader))\n",
    "print(\"model2:\", eval(model2, vox_dataloader))\n",
    "print(\"model:\", eval(model, vox_dataloader))\n",
    "\n",
    "print(\"vctk:\")\n",
    "print(\"model1:\", eval(model1, vctk_dataloader))\n",
    "print(\"model2:\", eval(model2, vctk_dataloader))\n",
    "print(\"model:\", eval(model, vctk_dataloader))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save({'model': model.state_dict()}, \"/home/yrb/code/ID-DEID/data/vae_model/vae_on_voxceleb.ckpt\")\n",
    "torch.save({'model': model.state_dict()}, \"/home/yrb/code/ID-DEID/data/vae_model/vae_wo_cos_on_voxceleb.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing the data loader...\n",
      "Finished loading the dataset...\n",
      "Finished loading the dataset...\n",
      "Finished loading the dataset...\n",
      "Start training VAE...\n",
      "\tEpoch 1 complete! \tAverage Loss:  117.84901721660907 \tAverage reconst Loss:  99.29551227276141 \tAverage KLD Loss:  18.55350538400503\n",
      "\tEpoch 2 complete! \tAverage Loss:  98.447020310622 \tAverage reconst Loss:  79.60963146503155 \tAverage KLD Loss:  18.837389139028694\n",
      "\tEpoch 3 complete! \tAverage Loss:  87.93128556471605 \tAverage reconst Loss:  69.55970294658954 \tAverage KLD Loss:  18.371582177969124\n",
      "\tEpoch 4 complete! \tAverage Loss:  82.50406411977914 \tAverage reconst Loss:  64.76671336247371 \tAverage KLD Loss:  17.737350463867188\n",
      "\tEpoch 5 complete! \tAverage Loss:  78.49950643686148 \tAverage reconst Loss:  61.14552483191857 \tAverage KLD Loss:  17.35398167830247\n",
      "Finish!!\n"
     ]
    }
   ],
   "source": [
    "model = VAE(x_dim, hidden_dim, latent_dim, DEVICE).to(DEVICE)\n",
    "checkpoint = torch.load(\"/home/yrb/code/ID-DEID/data/vae_model/l1_vae_on_voxceleb.ckpt\", map_location=DEVICE)\n",
    "model.load_state_dict(checkpoint['model'])\n",
    "\n",
    "cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "mse = nn.MSELoss(reduction='sum')\n",
    "l1 = nn.L1Loss(reduction='sum')\n",
    "def loss_function(x, x_hat, mean, log_var):\n",
    "    # cosine similarity loss\n",
    "    cos_distance_loss = 100*(1-cos(x, x_hat)).sum()\n",
    "    mse_loss = mse(x, x_hat)\n",
    "    reconstruction_loss = cos_distance_loss + mse_loss\n",
    "    KLD = - 0.5 * torch.sum(1 + log_var - mean.pow(2) - log_var.exp())\n",
    "    return reconstruction_loss, KLD\n",
    "\n",
    "def loss_function2(x, x_hat, mean, log_var):\n",
    "    # cosine similarity loss\n",
    "    cos_distance_loss = 200*(1-cos(x, x_hat)).sum()\n",
    "    l1_loss = l1(x, x_hat)\n",
    "    reconstruction_loss = cos_distance_loss + l1_loss\n",
    "    KLD = - 0.5 * torch.sum(1 + log_var - mean.pow(2) - log_var.exp())\n",
    "    return reconstruction_loss, KLD\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=lr*0.1)\n",
    "\n",
    "print(\"Preparing the data loader...\")\n",
    "vox_dataloader = get_loader(pkl_path=\"/home/yrb/data/ID-DEID_data/voxceleb_spmel/train.pkl\", num_workers=2, batch_size=batch_size)\n",
    "wsj_dataloader = get_loader(pkl_path=\"/home/yrb/data/ID-DEID_data/wsj_spmel/train.pkl\", num_workers=2, batch_size=batch_size)\n",
    "vctk_dataloader = get_loader(pkl_path=\"/home/yrb/data/ID-DEID_data/vctk_spmel/train.pkl\", num_workers=2, batch_size=batch_size)\n",
    "\n",
    "dataloader = wsj_dataloader\n",
    "\n",
    "print(\"Start training VAE...\")\n",
    "model.train()\n",
    "\n",
    "finetune_epochs = 5\n",
    "for epoch in range(finetune_epochs):\n",
    "    overall_loss, overall_reconst, overall_KLD = 0, 0, 0\n",
    "    for batch_idx, (spkrids, x) in enumerate(dataloader):\n",
    "        x = x.to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        x_hat, mean, log_var = model(x)\n",
    "        reconst_loss, KLD = loss_function2(x, x_hat, mean, log_var)\n",
    "        loss = reconst_loss + KLD\n",
    "        overall_loss += loss.item()\n",
    "        overall_reconst += reconst_loss.item()\n",
    "        overall_KLD += KLD.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(\"\\tEpoch\", epoch + 1, \"complete!\",\n",
    "        \"\\tAverage Loss: \", overall_loss / (batch_idx*batch_size),\n",
    "        \"\\tAverage reconst Loss: \", overall_reconst / (batch_idx*batch_size),\n",
    "        \"\\tAverage KLD Loss: \", overall_KLD / (batch_idx*batch_size),)\n",
    "\n",
    "print(\"Finish!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate emb from noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-0.0102,  0.0700,  0.0442, -0.0299, -0.0095, -0.1873, -0.2445,  0.0364,\n",
       "         -0.1090,  0.0463,  0.2157,  0.0664,  0.0020,  0.1663, -0.1197, -0.2575,\n",
       "         -0.0822, -0.1179,  0.0825, -0.1582, -0.0775,  0.0583, -0.1722,  0.0544,\n",
       "          0.0935,  0.1655, -0.0128,  0.2362, -0.0468,  0.0649, -0.1068, -0.1868],\n",
       "        device='cuda:0'),\n",
       " tensor(0.2362, device='cuda:0'),\n",
       " tensor(-0.0164, device='cuda:0'))"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    noise = torch.randn(batch_size, 64).to(DEVICE)\n",
    "    generated_emb = model1.Decoder(noise)\n",
    "\n",
    "cos_sim = cos(generated_emb, x[4:5, :])\n",
    "cos_sim, cos_sim.max(), cos_sim.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "19c6557b8642ad53c96fc5567ab2832fa0c5cbeae14f00371bdd2b5617c4bb1b"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('ID-DEID_ENV': virtualenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
