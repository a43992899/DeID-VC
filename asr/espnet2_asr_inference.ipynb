{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NZQgGCUgFlIm"
      },
      "source": [
        "# ESPnet2 ASR Inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vS6mnV_MFm-i"
      },
      "source": [
        "## 1.1 Connect to Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1HAvOly4FlXq",
        "outputId": "15fb71de-0f13-4bdf-c7e4-f2bbc382e96c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "# Connect Google drive\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\", force_remount=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IMHi6BW-FuKe"
      },
      "source": [
        "## 1.2 Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7DlhpxXXFlaQ"
      },
      "outputs": [],
      "source": [
        "!pip install -q espnet_model_zoo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "6MGQVK_TFlc2"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import torch\n",
        "import string\n",
        "import tarfile\n",
        "import pandas as pd\n",
        "import soundfile\n",
        "import numpy as np\n",
        "from csv import reader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HBaSAtULtBiP",
        "outputId": "0271d65b-0049-4950-c3c0-c3e44b9750b8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /home/ubuntu/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package cmudict to /home/ubuntu/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/cmudict.zip.\n"
          ]
        }
      ],
      "source": [
        "from espnet_model_zoo.downloader import ModelDownloader\n",
        "from espnet2.bin.asr_inference import Speech2Text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WwRXRDqZY2On"
      },
      "source": [
        "## 1.3 Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "K7SqdjfYFliW"
      },
      "outputs": [],
      "source": [
        "def text_normalizer(text):\n",
        "    text = text.upper()\n",
        "    return text.translate(str.maketrans('', '', string.punctuation))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "SEymO7kov8Qf"
      },
      "outputs": [],
      "source": [
        "def compute_wer(hyp_sentence=\"\",ref_sentence=\"\"):\n",
        "    \"\"\"\n",
        "    Inputs: \n",
        "    hyp_sentence: str- Sentence of text from the ASR Hypothesis\n",
        "    ref_sentence: str-Sentence of text from the Ground Truth Reference\n",
        "    Returns:\n",
        "    wer_score: float- WER Score as a floating point number rounded to two decimal places       \n",
        "    \"\"\"\n",
        "    ## Fill your code here\n",
        "    hyp_word = hyp_sentence.split()\n",
        "    ref_word = ref_sentence.split()\n",
        "\n",
        "    m = len(hyp_word)\n",
        "    n = len(ref_word)\n",
        "\n",
        "    w_table = np.zeros([m + 1, n + 1])\n",
        "    w_table[0, :] = np.arange(n + 1)\n",
        "    w_table[:, 0] = np.arange(m + 1)\n",
        "\n",
        "    for i in range(1, m + 1):\n",
        "        for j in range(1, n + 1):\n",
        "            if hyp_word[i - 1] == ref_word[j - 1]:\n",
        "                w_table[i, j] = w_table[i - 1, j - 1]\n",
        "            else:\n",
        "                w_table[i, j] = 1 + min(w_table[i - 1, j], w_table[i, j - 1], w_table[i - 1, j - 1])\n",
        "\n",
        "    score = w_table[m, n] / n\n",
        "    return score * 100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fHaRLF_Y7EG6"
      },
      "source": [
        "## 1.4 Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "cTbM-oXBeIgB"
      },
      "outputs": [],
      "source": [
        "#@title Choose English ASR model { run: \"auto\" }\n",
        "# Change and copy parameter on the right\n",
        "\n",
        "fs = 16000 #@param {type:\"integer\"}\n",
        "tag = 'kamo-naoyuki/wsj' #@param [\"Shinji Watanabe/spgispeech_asr_train_asr_conformer6_n_fft512_hop_length256_raw_en_unnorm_bpe5000_valid.acc.ave\", \"kamo-naoyuki/librispeech_asr_train_asr_conformer6_n_fft512_hop_length256_raw_en_bpe5000_scheduler_confwarmup_steps40000_optim_conflr0.0025_sp_valid.acc.ave\", \"kamo-naoyuki/wsj\"] {type:\"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EgWhbHmUFlfd",
        "outputId": "0f202ed2-618f-4ee8-bdd0-8b38d1f13d47"
      },
      "outputs": [],
      "source": [
        "d = ModelDownloader()\n",
        "speech2text = Speech2Text(\n",
        "    **d.download_and_unpack(tag),\n",
        "    # device=\"cuda\",\n",
        "    minlenratio=0.0,\n",
        "    maxlenratio=0.0,\n",
        "    ctc_weight=0.3,\n",
        "    beam_size=10,\n",
        "    batch_size=0,\n",
        "    nbest=1\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "giZz_ttJqRAK"
      },
      "source": [
        "## 2.1 Unzip Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RQYTakqVFlk7",
        "outputId": "5eb9410b-ab41-4088-b012-d34477447c9a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/gdrive/MyDrive/CMU/18781_Speech_Rec/data\n"
          ]
        }
      ],
      "source": [
        "%cd /content/gdrive/MyDrive/CMU/18781_Speech_Rec/data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ava4Xg6XqUiN"
      },
      "outputs": [],
      "source": [
        "# Untar LDC93S6B (csr_1_senn) (wsj0)\n",
        "filename = \"LDC93S6B.tgz\"\n",
        "tf = tarfile.open(filename)\n",
        "tf.extractall('/content')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tTZTHngBqUk2"
      },
      "outputs": [],
      "source": [
        "# Untar LDC94S13B (csr_senn) (wsj1)\n",
        "filename = \"LDC94S13B.tgz\"\n",
        "tf = tarfile.open(filename)\n",
        "tf.extractall('/content')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XgoZZGAwqiew"
      },
      "source": [
        "## 2.2 Read Data and Evaluate ASR"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uhPwKl2G1a1x"
      },
      "source": [
        "### 2.2.1 WSJ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7B8FQsNoqUnZ"
      },
      "outputs": [],
      "source": [
        "# LDC93S6B (csr_1_senn) (wsj0)\n",
        "with open('/content/gdrive/MyDrive/CMU/18781_Speech_Rec/csr_1_senn.csv', 'r') as read_obj:\n",
        "    csv_reader = reader(read_obj)\n",
        "    for row in csv_reader:\n",
        "        info = row[0]\n",
        "        path, id, ref_text = info.split('|')\n",
        "        path = \"/\".join(path.split('/')[4:])\n",
        "        speech, rate = soundfile.read(\"/content/csr_1_senn/\" + path)\n",
        "\n",
        "        nbests = speech2text(speech)\n",
        "        hyp_text, *_ = nbests[0]\n",
        "        \n",
        "        hyp_text = text_normalizer(hyp_text)\n",
        "        ref_text = text_normalizer(ref_text)\n",
        "\n",
        "        print(f\"Input Speech: /content/csr_1_senn/{path}\")\n",
        "        print(f\"Reference text: {ref_text}\")\n",
        "        print(f\"ASR hypothesis: {hyp_text}\")\n",
        "        print(f\"Score: {compute_wer(hyp_text, ref_text)}\")\n",
        "        print(\"*\" * 50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qiLygEVounr4"
      },
      "outputs": [],
      "source": [
        "# LDC94S13B (csr_senn) (wsj1)\n",
        "with open('/content/gdrive/MyDrive/CMU/18781_Speech_Rec/csr_senn.csv', 'r') as read_obj:\n",
        "    csv_reader = reader(read_obj)\n",
        "    for row in csv_reader:\n",
        "        info = row[0]\n",
        "        path, id, ref_text = info.split('|')\n",
        "        path = \"/\".join(path.split('/')[4:])\n",
        "        speech, rate = soundfile.read(\"/content/csr_senn/\" + path)\n",
        "\n",
        "        nbests = speech2text(speech)\n",
        "        hyp_text, *_ = nbests[0]\n",
        "        \n",
        "        hyp_text = text_normalizer(hyp_text)\n",
        "        ref_text = text_normalizer(ref_text)\n",
        "\n",
        "        print(f\"Input Speech: /content/csr_senn/{path}\")\n",
        "        print(f\"Reference text: {ref_text}\")\n",
        "        print(f\"ASR hypothesis: {hyp_text}\")\n",
        "        print(f\"Score: {compute_wer(hyp_text, ref_text)}\")\n",
        "        print(\"*\" * 50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PSuCE8-B1dRR"
      },
      "source": [
        "### 2.2.2 Single Audio File"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FoJl7GM11hou",
        "outputId": "5519a42a-24a5-47b1-fe25-de18619329df"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ASR hypothesis: PRICES COULD FALL FURTHER BARRING A PERSIAN GULF WAR\n"
          ]
        }
      ],
      "source": [
        "# Example\n",
        "speech, rate = soundfile.read(\"/home/ubuntu/mnt/jl/ID-DEID/data/wav/4k0/4k0a010a.wv1.wav\")\n",
        "nbests = speech2text(speech)\n",
        "\n",
        "hyp_text, *_ = nbests[0]\n",
        "# ref_text = \"IT WILL NOT BE SAFE FOR YOU TO STAY HERE NOW\"\n",
        "# ref_text = \"IT WILL BE NO DISAPPOINTMENT TO ME\"\n",
        "\n",
        "hyp_text = text_normalizer(hyp_text)\n",
        "# ref_text = text_normalizer(ref_text)\n",
        "\n",
        "# print(f\"Reference text: {ref_text}\")\n",
        "print(f\"ASR hypothesis: {hyp_text}\")\n",
        "# print(f\"Score: {compute_wer(hyp_text, ref_text)}\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "giZz_ttJqRAK",
        "uhPwKl2G1a1x"
      ],
      "name": "espnet2_asr_inference.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "f06dbfe05a6cd680013574df72362038b0d1f34ab273963fedc00298e24ce9e1"
    },
    "kernelspec": {
      "display_name": "Python 3.6.9 64-bit ('ID-DEID_3.6_ENV': virtualenv)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
